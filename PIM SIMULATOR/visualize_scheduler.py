import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from Q_Learning.adaptive_scheduler import AdaptiveQLearningScheduler   
def run_simulation_and_visualize():
    print("ğŸš€ SimÃ¼lasyon ve GÃ¶rselleÅŸtirme BaÅŸlÄ±yor...")
    
    # 1. Scheduler'Ä± BaÅŸlat
    scheduler = AdaptiveQLearningScheduler(
        learning_rate=0.1,
        epsilon=0.9,       # YÃ¼ksek baÅŸlatÄ±p dÃ¼ÅŸÃ¼receÄŸiz
        discount_factor=0.9,
        energy_weight=0.7,
        latency_weight=0.3
    )
    
    # 2. EÄŸitim Verisi OluÅŸtur (Ã‡eÅŸitli senaryolar)
    # (Workload Size, Layer Type, Deadline ms)
    scenarios = [
        (50000, 'Conv', 5.0),       # KÃ¼Ã§Ã¼k, Strict -> PIM seÃ§meli
        (15000000, 'FC', 100.0),    # BÃ¼yÃ¼k, Relaxed -> GPU seÃ§meli
        (500000, 'ReLU', 20.0),     # Orta -> Hybrid olabilir
        (80000, 'Conv', None),      # KÃ¼Ã§Ã¼k, No Deadline
        (20000000, 'Conv', 10.0),   # Ã‡ok bÃ¼yÃ¼k, Strict -> GPU (HÄ±z lazÄ±m)
    ]
    
    # 3. EÄŸitim DÃ¶ngÃ¼sÃ¼
    n_episodes = 300
    rewards_per_episode = []
    epsilon_history = []
    
    for episode in range(n_episodes):
        # Her episode'da senaryolarÄ± karÄ±ÅŸtÄ±rÄ±p eÄŸit
        episode_data = scenarios * 2  # Veriyi Ã§oÄŸalt
        np.random.shuffle(episode_data)
        
        # Senin sÄ±nÄ±fÄ±ndaki train_episode metodunu Ã§aÄŸÄ±r
        total_reward = scheduler.train_episode(None, None, episode_data)
        
        rewards_per_episode.append(total_reward)
        epsilon_history.append(scheduler.epsilon)
    
    # 4. Verileri GÃ¶rselleÅŸtirme iÃ§in HazÄ±rla
    
    # Grafik Ã‡erÃ§evesi AyarlarÄ±
    sns.set(style="whitegrid")
    fig, axes = plt.subplots(2, 2, figsize=(18, 12))
    fig.suptitle('Q-Learning Scheduler Performans Analizi', fontsize=20, weight='bold')
    
    # --- GRAFÄ°K 1: Ã–ÄŸrenme EÄŸrisi (Total Reward) ---
    # Moving Average ile gÃ¼rÃ¼ltÃ¼yÃ¼ azaltarak Ã§izelim
    series_reward = pd.Series(rewards_per_episode)
    window_size = 20
    rolling_mean = series_reward.rolling(window=window_size).mean()
    
    axes[0, 0].plot(rewards_per_episode, alpha=0.3, color='gray', label='Raw Reward')
    axes[0, 0].plot(rolling_mean, color='blue', linewidth=2, label=f'{window_size}-Ep Mov. Avg')
    axes[0, 0].set_title('Ã–ÄŸrenme EÄŸrisi (Convergence)', fontsize=14)
    axes[0, 0].set_xlabel('Episode')
    axes[0, 0].set_ylabel('Total Reward')
    axes[0, 0].legend()
    
    # --- GRAFÄ°K 2: Epsilon Decay (Exploration vs Exploitation) ---
    axes[0, 1].plot(epsilon_history, color='orange', linewidth=2)
    axes[0, 1].set_title('Epsilon Decay (KeÅŸif OranÄ±)', fontsize=14)
    axes[0, 1].set_xlabel('Episode')
    axes[0, 1].set_ylabel('Epsilon DeÄŸeri')
    axes[0, 1].text(n_episodes*0.7, 0.5, 'Exploration', fontsize=12, color='orange')
    axes[0, 1].text(n_episodes*0.7, 0.1, 'Exploitation', fontsize=12, color='green')
    
    # --- GRAFÄ°K 3: Q-Table IsÄ± HaritasÄ± (Policy Heatmap) ---
    # Q-table'Ä± DataFrame'e Ã§evirelim
    q_data = []
    for state, values in scheduler.q_table.items():
        state_str = f"{state[0]}\n{state[1]}\n{state[2]}" # (Size, Layer, Deadline)
        row = {
            'State': state_str,
            'PIM': values[0],
            'GPU': values[1],
            'HYBRID': values[2]
        }
        q_data.append(row)
    
    df_q = pd.DataFrame(q_data).set_index('State')
    
    # Sadece en Ã§ok karÅŸÄ±laÅŸÄ±lan 10 durumu gÃ¶ster (tablo Ã§ok bÃ¼yÃ¼kse)
    if len(df_q) > 10:
        df_q = df_q.head(10)
        
    sns.heatmap(df_q, annot=True, cmap='RdYlGn', fmt='.1f', linewidths=.5, ax=axes[1, 0])
    axes[1, 0].set_title('Q-Table IsÄ± HaritasÄ± (Tercih Edilen Aksiyonlar)', fontsize=14)
    axes[1, 0].set_ylabel('State (Size, Layer, Deadline)')
    
    # --- GRAFÄ°K 4: Workload BazlÄ± Karar DaÄŸÄ±lÄ±mÄ± ---
    # Modeli test edip hangi boyutta ne seÃ§tiÄŸine bakalÄ±m
    test_results = {'Size': [], 'Action': []}
    test_sizes = [50000, 500000, 5000000, 15000000] # KÃ¼Ã§Ã¼kten bÃ¼yÃ¼ÄŸe
    test_layers = ['Conv', 'FC']
    
    for size in test_sizes:
        for layer in test_layers:
            # Deadline'Ä± relaxed tutalÄ±m ki salt size etkisine bakalÄ±m
            action, _ = scheduler.predict(size, layer, deadline_ms=100)
            
            # Kategorik isimlendirme (grafik iÃ§in)
            if size < 100000: cat = 'Small'
            elif size < 10000000: cat = 'Medium'
            else: cat = 'Large'
            
            test_results['Size'].append(cat)
            test_results['Action'].append(action)

    df_test = pd.DataFrame(test_results)
    
    sns.countplot(x='Size', hue='Action', data=df_test, ax=axes[1, 1], palette='viridis')
    axes[1, 1].set_title('Workload Boyutuna GÃ¶re Karar DaÄŸÄ±lÄ±mÄ±', fontsize=14)
    axes[1, 1].set_ylabel('SeÃ§im SayÄ±sÄ±')

    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    run_simulation_and_visualize()