"""
Q-Learning Based Adaptive Scheduler for PIM/GPU Hybrid System

Bu modÃ¼l, iÅŸ yÃ¼kÃ¼ Ã¶zelliklerine gÃ¶re PIM veya GPU seÃ§imini Ã¶ÄŸrenir.
GeÃ§miÅŸ deneyimlerden reward alarak optimal kararlar verir.

State: (workload_size_category, layer_type, deadline_constraint)
Action: {PIM, GPU, HYBRID}
Reward: energy_saved - latency_penalty
"""

import numpy as np
import json
from collections import defaultdict

class AdaptiveQLearningScheduler:
    """
    Q-Learning tabanlÄ± akÄ±llÄ± scheduler.

    Ã–ÄŸrenme sÃ¼reci:
    1. BaÅŸlangÄ±Ã§ta rastgele kararlar verir (exploration)
    2. Her karar sonrasÄ± reward alÄ±r (enerji tasarrufu - gecikme cezasÄ±)
    3. Q-table gÃ¼ncellenir
    4. Zaman iÃ§inde optimal stratejiyi Ã¶ÄŸrenir (exploitation)
    """

    def __init__(self,
                 learning_rate=0.1,
                 discount_factor=0.9,
                 epsilon=0.3,
                 energy_weight=0.7,
                 latency_weight=0.3):
        """
        Args:
            learning_rate (alpha): Q-value gÃ¼ncelleme hÄ±zÄ± [0-1]
            discount_factor (gamma): Gelecek reward'larÄ±n Ã¶nemi [0-1]
            epsilon: Exploration oranÄ± (rastgele aksiyon alma) [0-1]
            energy_weight: Enerji tasarrufunun reward'daki aÄŸÄ±rlÄ±ÄŸÄ±
            latency_weight: Gecikme cezasÄ±nÄ±n reward'daki aÄŸÄ±rlÄ±ÄŸÄ±
        """
        # Hyperparameters
        self.alpha = learning_rate
        self.gamma = discount_factor
        self.epsilon = epsilon  # Exploration rate
        
        self.energy_weight = energy_weight
        self.latency_weight = latency_weight
        
        # Q-Table: Q(state, action) -> expected reward
        # State: (workload_category, layer_type, deadline_category)
        # Action: 0=PIM, 1=GPU, 2=HYBRID
        self.q_table = defaultdict(lambda: np.zeros(3))
        
        # Action mapping
        self.actions = ['PIM', 'GPU', 'HYBRID']
        self.action_to_idx = {a: i for i, a in enumerate(self.actions)}
        
        # Learning statistics
        self.episode_count = 0
        self.total_reward = 0
        self.action_history = []
        self.reward_history = []

    def _discretize_state(self, workload_size, layer_type, deadline_ms=None):
        """
        Continuous state'i discrete kategorilere dÃ¶nÃ¼ÅŸtÃ¼r.
        
        Args:
            workload_size: Ä°ÅŸlem sayÄ±sÄ±
            layer_type: 'Conv', 'FC', 'ReLU', vs.
            deadline_ms: Gecikme kÄ±sÄ±tÄ± (ms)
        
        Returns:
            state: (workload_cat, layer_type, deadline_cat) tuple
        """
        # Workload kategorisi
        if workload_size < 100000:
            workload_cat = 'small'
        elif workload_size < 10000000:
            workload_cat = 'medium'
        else:
            workload_cat = 'large'
        
        # Layer type (doÄŸrudan kullan)
        layer_cat = layer_type
        
        # Deadline kategorisi
        if deadline_ms is None:
            deadline_cat = 'none'
        elif deadline_ms < 10:
            deadline_cat = 'strict'  # <10ms (real-time)
        elif deadline_ms < 50:
            deadline_cat = 'moderate'  # 10-50ms
        else:
            deadline_cat = 'relaxed'  # >50ms
        
        return (workload_cat, layer_cat, deadline_cat)
    
    def select_action(self, state, training=True):
        """
        Epsilon-greedy policy ile action seÃ§.
        
        Args:
            state: Mevcut durum
            training: True ise exploration yapar
        
        Returns:
            action: 'PIM', 'GPU', veya 'HYBRID'
        """
        if training and np.random.random() < self.epsilon:
            # Exploration: Rastgele action seÃ§
            action_idx = np.random.choice(3)
        else:
            # Exploitation: En yÃ¼ksek Q-value'lu action seÃ§
            q_values = self.q_table[state]
            action_idx = np.argmax(q_values)
        
        return self.actions[action_idx]
    
    def calculate_reward(self, energy_pim, energy_gpu, 
                        latency_pim, latency_gpu, 
                        chosen_action, deadline_ms=None):
        """
        Reward fonksiyonu: enerji tasarrufu - gecikme cezasÄ±
        
        Reward yapÄ±sÄ±:
        - Enerji tasarrufu: pozitif reward
        - Deadline ihlali: bÃ¼yÃ¼k negatif reward
        - Gereksiz yavaÅŸlÄ±k: kÃ¼Ã§Ã¼k negatif reward
        
        Args:
            energy_pim, energy_gpu: PIM ve GPU enerjisi (mJ)
            latency_pim, latency_gpu: PIM ve GPU gecikmesi (ms)
            chosen_action: SeÃ§ilen aksiyon
            deadline_ms: Gecikme kÄ±sÄ±tÄ±
        
        Returns:
            reward: Toplam reward deÄŸeri
        """
        # SeÃ§ilen aksiyonun maliyetleri
        if chosen_action == 'PIM':
            actual_energy = energy_pim
            actual_latency = latency_pim
        elif chosen_action == 'GPU':
            actual_energy = energy_gpu
            actual_latency = latency_gpu
        else:  # HYBRID
            actual_energy = (energy_pim * 0.7 + energy_gpu * 0.3)
            actual_latency = max(latency_pim * 0.7, latency_gpu * 0.3)
        
        # Enerji tasarrufu (GPU baseline'a gÃ¶re)
        energy_saving = (energy_gpu - actual_energy) / energy_gpu
        energy_reward = energy_saving * self.energy_weight * 100
        
        # Gecikme cezasÄ±
        latency_penalty = 0
        
        if deadline_ms is not None:
            if actual_latency > deadline_ms:
                # Deadline ihlali: bÃ¼yÃ¼k ceza
                violation_ratio = (actual_latency - deadline_ms) / deadline_ms
                latency_penalty = violation_ratio * 200  # BÃ¼yÃ¼k ceza
            else:
                # Deadline iÃ§inde ama ne kadar hÄ±zlÄ±?
                # GPU'dan ne kadar yavaÅŸ?
                slowdown = (actual_latency - latency_gpu) / latency_gpu
                latency_penalty = slowdown * self.latency_weight * 50
        else:
            # Deadline yok, sadece gÃ¶receli yavaÅŸlÄ±ÄŸa bak
            slowdown = (actual_latency - latency_gpu) / latency_gpu
            latency_penalty = slowdown * self.latency_weight * 30
        
        # Toplam reward
        reward = energy_reward - latency_penalty
        
        return reward
    
    def update_q_table(self, state, action, reward, next_state):
        """
        Q-Learning gÃ¼ncelleme kuralÄ±:
        Q(s,a) â† Q(s,a) + Î±[r + Î³Â·max(Q(s',a')) - Q(s,a)]
        
        Args:
            state: Ã–nceki durum
            action: YapÄ±lan aksiyon
            reward: AlÄ±nan reward
            next_state: Yeni durum
        """
        action_idx = self.action_to_idx[action]
        
        # Current Q-value
        current_q = self.q_table[state][action_idx]
        
        # Max Q-value for next state
        max_next_q = np.max(self.q_table[next_state])
        
        # Q-learning update
        new_q = current_q + self.alpha * (reward + self.gamma * max_next_q - current_q)
        
        self.q_table[state][action_idx] = new_q
        
        # Statistics
        self.reward_history.append(reward)
        self.total_reward += reward
    
    # â¬‡ Ã–NEMLÄ°: Bu fonksiyonlar update_q_table ile AYNI SEVÄ°YEDE olmalÄ±!
    def train_episode(self, pim_simulator, gpu_simulator, workload_data):
        """
        Bir eÄŸitim episode'u Ã§alÄ±ÅŸtÄ±r.
        
        Args:
            pim_simulator: PIM simÃ¼latÃ¶rÃ¼
            gpu_simulator: GPU simÃ¼latÃ¶rÃ¼
            workload_data: [(workload_size, layer_type, deadline), ...]
        
        Returns:
            episode_reward: Episode toplam reward'u
        """
        episode_reward = 0
        
        for workload_size, layer_type, deadline in workload_data:
            # State'i discretize et
            state = self._discretize_state(workload_size, layer_type, deadline)
            
            # Action seÃ§ (epsilon-greedy)
            action = self.select_action(state, training=True)
            
            # SimÃ¼le et (basitleÅŸtirilmiÅŸ)
            # GerÃ§ek sistemde: pim_simulator.process(workload)
            energy_pim = workload_size * 0.0001  # Ã–rnek deÄŸer
            energy_gpu = workload_size * 0.00015
            latency_pim = workload_size * 0.00002
            latency_gpu = workload_size * 0.000003
            
            # Reward hesapla
            reward = self.calculate_reward(
                energy_pim, energy_gpu,
                latency_pim, latency_gpu,
                action, deadline
            )
            
            # Next state (basit: aynÄ± state)
            next_state = state
            
            # Q-table gÃ¼ncelle
            self.update_q_table(state, action, reward, next_state)
            
            episode_reward += reward
            self.action_history.append((state, action))
        
        self.episode_count += 1
        
        # Epsilon decay (zamanla exploration azalt)
        self.epsilon = max(0.05, self.epsilon * 0.995)
        
        return episode_reward
    
    def predict(self, workload_size, layer_type, deadline_ms=None):
        """
        Ã–ÄŸrenilmiÅŸ policy ile en iyi aksiyonu seÃ§ (inference mode).
        
        Args:
            workload_size: Ä°ÅŸlem sayÄ±sÄ±
            layer_type: Katman tipi
            deadline_ms: Gecikme kÄ±sÄ±tÄ±
        
        Returns:
            action: 'PIM', 'GPU', veya 'HYBRID'
            confidence: Q-value (ne kadar emin?)
        """
        state = self._discretize_state(workload_size, layer_type, deadline_ms)
        q_values = self.q_table[state]
        
        best_action_idx = np.argmax(q_values)
        best_action = self.actions[best_action_idx]
        confidence = q_values[best_action_idx]
        
        return best_action, confidence
    
    def get_statistics(self):
        """Ã–ÄŸrenme istatistikleri"""
        return {
            'episodes': self.episode_count,
            'total_reward': self.total_reward,
            'avg_reward': self.total_reward / max(1, self.episode_count),
            'q_table_size': len(self.q_table),
            'epsilon': self.epsilon,
            'recent_rewards': self.reward_history[-10:] if self.reward_history else []
        }
    
    def save_model(self, filepath='q_learning_model.json'):
        """Q-table'Ä± kaydet"""
        model_data = {
            'q_table': {str(k): v.tolist() for k, v in self.q_table.items()},
            'hyperparameters': {
                'alpha': self.alpha,
                'gamma': self.gamma,
                'epsilon': self.epsilon,
                'energy_weight': self.energy_weight,
                'latency_weight': self.latency_weight
            },
            'statistics': self.get_statistics()
        }
        
        with open(filepath, 'w') as f:
            json.dump(model_data, f, indent=2)
        
        print(f"âœ…Model kaydedildi: {filepath}")
    
    def load_model(self, filepath='q_learning_model.json'):
        """Q-table'Ä± yÃ¼kle"""
        with open(filepath, 'r') as f:
            model_data = json.load(f)
        
        # Q-table'Ä± yÃ¼kle
        self.q_table = defaultdict(lambda: np.zeros(3))
        for state_str, q_values in model_data['q_table'].items():
            state = eval(state_str)  # String'i tuple'a Ã§evir
            self.q_table[state] = np.array(q_values)
        
        # Hyperparameters
        hyper = model_data['hyperparameters']
        self.alpha = hyper['alpha']
        self.gamma = hyper['gamma']
        self.epsilon = hyper['epsilon']
        self.energy_weight = hyper['energy_weight']
        self.latency_weight = hyper['latency_weight']
        
        print(f" Model yÃ¼klendi: {filepath}")
        print(f"   Q-table boyutu: {len(self.q_table)} state")
        print(f"   Epsilon: {self.epsilon:.3f}")


# KullanÄ±m Ã¶rneÄŸi (standalone test)
if __name__ == "__main__":
    print(" Q-Learning Scheduler Demo")
    print("="*60)
    
    # Scheduler oluÅŸtur
    scheduler = AdaptiveQLearningScheduler(
        learning_rate=0.1,
        epsilon=0.3,
        energy_weight=0.7,
        latency_weight=0.3
    )
    
    # Ã–rnek training data
    training_workloads = [
        (50000, 'Conv', 50.0),
        (5000000, 'Conv', 100.0),
        (100000, 'ReLU', 10.0),
        (10000000, 'FC', 50.0),
        (1000000, 'Conv', 20.0),
    ]
    
    # 10 episode eÄŸit
    print("\n Q-Learning EÄŸitimi BaÅŸlÄ±yor...")
    for episode in range(10):
        reward = scheduler.train_episode(None, None, training_workloads * 5)
        print(f"Episode {episode+1}: Reward = {reward:.2f}, Epsilon = {scheduler.epsilon:.3f}")
    
    # Ä°statistikler
    print("\n Ã–ÄŸrenme Ä°statistikleri:")
    stats = scheduler.get_statistics()
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    # Inference
    print("\nğŸ” Test Predictions:")
    test_cases = [
        (50000, 'Conv', None),
        (10000000, 'FC', 10.0),
        (1000000, 'ReLU', 50.0)
    ]
    
    for workload, layer, deadline in test_cases:
        action, confidence = scheduler.predict(workload, layer, deadline)
        print(f"  Workload={workload}, Layer={layer}, Deadline={deadline}")
        print(f"    â†’ {action} (confidence: {confidence:.2f})")
    
    # Modeli kaydet
    scheduler.save_model('q_learning_scheduler.json')