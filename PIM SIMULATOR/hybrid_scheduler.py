import numpy as np

class HybridSystem:
    """
    PIM ve GPU arasÄ±nda hem iÅŸ yÃ¼kÃ¼ tabanlÄ± hem de katman tabanlÄ± (Layer-wise)
    daÄŸÄ±lÄ±m yapan GeliÅŸmiÅŸ Hibrit ZamanlayÄ±cÄ±.
    """
    def __init__(self, pim_array, gpu_baseline):
        self.pim = pim_array
        self.gpu = gpu_baseline
        
        # Basit iÅŸ yÃ¼kÃ¼ eÅŸiÄŸi (Eski testlerin Ã§alÄ±ÅŸmasÄ± iÃ§in)
        self.workload_threshold = 50000 

        # Veri Transfer Maliyeti (PIM <-> GPU)
        # PCIe Ã¼zerinden veri aktarÄ±mÄ± maliyetlidir.
        # VarsayÄ±m: 0.05 mJ/MB enerji ve 1.0 ms/MB gecikme
        self.transfer_energy_per_mb = 0.05 
        self.transfer_latency_per_mb = 1.0 

    # --- 1. ESKÄ° TESTLER Ä°Ã‡Ä°N BASÄ°T MANTIK ---
    def adaptive_processing(self, total_macs, model=None):
        """Basit iÅŸ yÃ¼kÃ¼ boyutuna gÃ¶re karar verir."""
        # GPU Tahmini
        gpu_stats = self.gpu.model_inference(total_macs)
        
        # PIM Tahmini (Basit yaklaÅŸÄ±m)
        _, e_pj, l_ns = self.pim.clusters[0].mac_8bit(128, 128, precision=8)
        pim_energy = (total_macs * e_pj) / 1e9
        pim_latency = (total_macs / 256 * l_ns) / 1e6 # 256 cluster paralel

        if total_macs < self.workload_threshold:
            return pim_energy, pim_latency, "PIM"
        else:
            return gpu_stats['total_energy_mj'], gpu_stats['total_latency_ms'], "GPU"

    def benchmark_comparison(self):
        """Basit benchmark raporu."""
        workloads = [1000, 50000, 10000000]
        results = []
        for w in workloads:
            pe, pl, _ = self.adaptive_processing(w) # PIM varsayÄ±mÄ±
            gres = self.gpu.model_inference(w)
            ge, gl = gres['total_energy_mj'], gres['total_latency_ms']
            
            # Karar
            if w < self.workload_threshold:
                he, hl, d = pe, pl, "PIM"
            else:
                he, hl, d = ge, gl, "GPU"
                
            results.append({
                'workload': w, 'pim_energy': pe, 'pim_latency': pl,
                'gpu_energy': ge, 'gpu_latency': gl,
                'hybrid_energy': he, 'hybrid_latency': hl, 'decision': d
            })
        return results

    # --- 2. YENÄ° KATMAN BAZLI (LAYER-WISE) MANTIK ---
    
    def analyze_model_layers(self, model_layers, input_data_mb):
        """
        Modelin katmanlarÄ±nÄ± analiz eder ve her biri iÃ§in en uygun cihazÄ± seÃ§er.
        Veri transfer maliyetlerini de hesaba katar.
        """
        execution_plan = []
        current_device = 'PIM' # Veri baÅŸlangÄ±Ã§ta PIM'de (SensÃ¶r/Memory) varsayalÄ±m
        
        total_energy = 0
        total_latency = 0

        print(f"\nğŸ” Model Analizi BaÅŸlÄ±yor ({len(model_layers)} katman)...")

        for layer in model_layers:
            layer_name = layer['name']
            layer_type = layer['type']
            
            # Karar MantÄ±ÄŸÄ±
            decision = "GPU" # VarsayÄ±lan
            reason = ""

            # 1. Convolution KatmanlarÄ± -> PIM (Memory Bound)
            if layer_type == 'Conv2D':
                decision = 'PIM'
                reason = "Memory-intensive MAC operations"
            
            # 2. Linear (Fully Connected) -> GPU (Compute Bound)
            elif layer_type == 'Linear':
                decision = 'GPU'
                reason = "Large Matrix Multiplication"
            
            # 3. Aktivasyonlar -> PIM (LUT Friendly)
            elif layer_type in ['ReLU', 'Sigmoid']:
                decision = 'PIM'
                reason = "Simple LUT Operation"
            
            # --- Maliyet HesabÄ± ---
            
            # EÄŸer cihaz deÄŸiÅŸirse Transfer Maliyeti ekle
            transfer_cost_e = 0
            transfer_cost_l = 0
            
            if decision != current_device:
                transfer_cost_e = input_data_mb * self.transfer_energy_per_mb
                transfer_cost_l = input_data_mb * self.transfer_latency_per_mb
                reason += f" + Data Transfer ({current_device}->{decision})"
                current_device = decision # Cihaz deÄŸiÅŸti
            
            # Ä°ÅŸlem Maliyeti (SimÃ¼lasyon)
            if decision == 'PIM':
                # PIM Maliyeti (Conv layer fonksiyonunu kullanarak)
                if layer_type == 'Conv2D':
                    stats = self.pim.convolution_layer(layer['input'], layer['kernel'], precision=8)
                    layer_energy = stats['energy_total_mj']
                    layer_latency = stats['latency_ms']
                else:
                    # Basit iÅŸlemler iÃ§in Ã§ok dÃ¼ÅŸÃ¼k maliyet
                    layer_energy = 0.01
                    layer_latency = 0.005
            else:
                # GPU Maliyeti
                # Ä°ÅŸlem sayÄ±sÄ±nÄ± tahmin et (Conv ise kernel, Linear ise matrix boyutu)
                if layer_type == 'Conv2D':
                    macs = layer['kernel'][0] * layer['kernel'][1] * layer['kernel'][2] * layer['kernel'][3] * layer['input'][1] * layer['input'][2]
                elif layer_type == 'Linear':
                     macs = layer['in_features'] * layer['out_features']
                else:
                    macs = 1000 # Basit iÅŸlem
                
                stats = self.gpu.model_inference(macs)
                layer_energy = stats['total_energy_mj']
                layer_latency = stats['total_latency_ms']

            # ToplamlarÄ± GÃ¼ncelle
            total_energy += layer_energy + transfer_cost_e
            total_latency += layer_latency + transfer_cost_l

            execution_plan.append({
                'layer': layer_name,
                'type': layer_type,
                'device': decision,
                'energy': layer_energy + transfer_cost_e,
                'latency': layer_latency + transfer_cost_l,
                'reason': reason
            })

        return execution_plan, total_energy, total_latency